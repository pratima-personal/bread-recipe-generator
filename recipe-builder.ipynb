{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import plotly.express as px\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from nltk.corpus import stopwords\n",
    "import markovify\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordListToFreqDict(wordlist):\n",
    "    wordfreq = [wordlist.count(p) for p in wordlist]\n",
    "    return dict(list(zip(wordlist,wordfreq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort dictionary according to descending frequency of words in recipe and return top N results\n",
    "def sortedDict(worddict, N):\n",
    "    newdict = {k: v for k, v in sorted(worddict.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return dict(list(newdict.items())[:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    punctuation_map = str.maketrans('', '', string.punctuation)\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list.remove('i')\n",
    "    stopwords_list.remove('me')\n",
    "    stopwords_list.append('com')\n",
    "    stopwords_set = set(stopwords_list)\n",
    "    text = text.split()\n",
    "    text = [word for word in text if not ('http' in word or 'www' in word)]\n",
    "    text = [word.translate(punctuation_map).lower() for word in text]\n",
    "    tokenized_words = [word for word in text if word not in stopwords_set]\n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(tokens, length=21):\n",
    "    sequences = []\n",
    "    for i in range(length, len(tokens)+1):\n",
    "        seq = tokens[i-length:i]\n",
    "        line = ' '.join(seq)\n",
    "        sequences.append(line)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions from model\n",
    "def generate_sentence(model, tokenizer, sequence_length, starting_text, num_predicted_words):\n",
    "    prediction = [starting_text]\n",
    "    for _ in range(num_predicted_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([starting_text])[0]\n",
    "        encoded_text = pad_sequences([encoded_text], \n",
    "                                     maxlen=sequence_length, \n",
    "                                     truncating='pre')\n",
    "        preds = model.predict_classes(encoded_text, verbose=0)\n",
    "        out_word = ''\n",
    "        for word, idx in tokenizer.word_index.items():\n",
    "            if idx == preds:\n",
    "                out_word = word\n",
    "                break\n",
    "        starting_text += ' ' + out_word\n",
    "        prediction.append(out_word)\n",
    "    return ' '.join(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('data/recipeInfo.txt', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        data.append(line[:-2])\n",
    "        line = f.readline()\n",
    "f.close()\n",
    "\n",
    "df_recipe = pd.DataFrame(data, columns=['Recipe'])\n",
    "df_recipe['Length'] = df_recipe['Recipe'].apply(lambda x: len(x.split()))\n",
    "df_recipe['Unique Words'] = df_recipe['Recipe'].apply(lambda x: len(set(x.split())))\n",
    "df_recipe['Tokenized Recipe'] = df_recipe['Recipe'].apply(tokenize)\n",
    "df_recipe['Cleaned Recipe'] = df_recipe['Tokenized Recipe'].str.join(' ')\n",
    "df_recipe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "# join the different processed titles together.\n",
    "long_string = ','.join(list(df_recipe['Recipe'].values))\n",
    "# create a WordCloud object\n",
    "wordcloud = WordCloud(background_color='white', max_words=100, contour_width=3, contour_color='steelblue')\n",
    "# generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "# visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a histogram with word frequency counts, but without stop words\n",
    "\n",
    "recipe_split = [string.split(' ') for string in df_recipe['Recipe'].tolist()]\n",
    "word_list = [word.translate(str.maketrans('', '', string.punctuation)).lower() \n",
    "             for recipe in recipe_split for word in recipe]\n",
    "word_list = [word for word in word_list if word not in set(stopwords.words('english'))]\n",
    "word_dict = sortedDict( wordListToFreqDict(word_list), 21 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(word_dict.items())[1:], columns=['Word', 'Frequency'])\n",
    "fig = px.bar(df, x=df['Word'], y=df['Frequency'], orientation='v')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing LDA for topic modeling\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "count_data = count_vectorizer.fit_transform(df_recipe['Cleaned Recipe'])\n",
    "\n",
    "number_topics = 5\n",
    "number_words = 15\n",
    "\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1, learning_method='batch', max_iter=50, random_state=42)\n",
    "lda.fit(count_data)\n",
    "\n",
    "# function to print n top words from each topics found by the LDA fit\n",
    "def print_topics_LDA(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('\\nTopic #%d:' % topic_idx)\n",
    "        print(' '.join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# print the topics found by the LDA model\n",
    "print('Topics found via LDA:')\n",
    "print_topics_LDA(lda, count_vectorizer, number_words)\n",
    "\n",
    "# assign topic to each recipe entry in dataframe\n",
    "topic_values = lda.transform(count_data)\n",
    "df_recipe['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize topics using pyLDAvis package\n",
    "import pyLDAvis.sklearn\n",
    " \n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda, count_data, count_vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing NMF for topic modeling\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = tfidf_vectorizer.fit_transform(df_recipe['Cleaned Recipe'])\n",
    "\n",
    "nmf = NMF(n_components=number_topics, random_state=42)\n",
    "nmf.fit(doc_term_matrix)\n",
    "\n",
    "# function to print n top words from each topics found by the NMF fit\n",
    "def print_topics_NMF(model, tfidf_vectorizer, n_top_words):\n",
    "    words = tfidf_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('\\nTopic #%d:' % topic_idx)\n",
    "        print(' '.join([words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "print('Topics found via NMF:')\n",
    "print_topics_NMF(nmf, tfidf_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Markov Chain model using markovify to predict text\n",
    "\n",
    "recipes = df_recipe['Cleaned Recipe'].tolist()\n",
    "text_model = markovify.NewlineText(recipes, state_size=2)\n",
    "for idx in range(2):\n",
    "    print(idx, text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSent(model, iters, minLength=1):\n",
    "  sentences = {}\n",
    "  for i in range(iters): \n",
    "    modelGen = model.chain.gen()\n",
    "    prevPrevWord = \"___BEGIN__\"\n",
    "    prevWord = next(modelGen)\n",
    "    madeSentence = prevWord + \" \"\n",
    "    \n",
    "    totalScore = 0\n",
    "    numWords = 1\n",
    "    for curWord in modelGen:\n",
    "      madeSentence += curWord + \" \"\n",
    "      numWords += 1\n",
    "      totalScore += model.chain.model[(prevPrevWord, prevWord)][curWord]\n",
    "      prevPrevWord = prevWord\n",
    "      prevWord = curWord\n",
    "    \n",
    "    madeSentence = madeSentence.strip()\n",
    "    if numWords == 0: continue\n",
    "    \n",
    "    if numWords < minLength: continue\n",
    "    if madeSentence in sentences: continue\n",
    "    \n",
    "    totalScore += model.chain.model[(prevPrevWord, prevWord)][\"___END__\"]\n",
    "    \n",
    "    sentences[madeSentence] = totalScore/float(numWords)\n",
    "  \n",
    "  # Get the sentences as (sentence, score) pairs and sort them so the sentences with the highest score appear first\n",
    "  sorted(sentences.items(), key=lambda x: -x[1])\n",
    "  \n",
    "  return sentences.items()\n",
    "\n",
    "list(getSent(text_model, 500, 4))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "recipes_token = df_recipe['Tokenized Recipe'].tolist()\n",
    "token_sequences = [token for recipe in recipes_token for \n",
    "                   token in get_sequence(recipe, length = seq_length+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('tokenized_recipes.txt', 'w')\n",
    "file.write('\\n'.join(token_sequences))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(token_sequences)\n",
    "num_sequences = tokenizer.texts_to_sequences(token_sequences)\n",
    "num_sequences = pad_sequences(num_sequences, padding='pre')\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_array = np.array(num_sequences)\n",
    "X, y = sequences_array[:,:-1], sequences_array[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, seq_length, input_length=seq_length))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './checkpoints/recipe_model.h5'\n",
    "checkpoint = ModelCheckpoint(path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(X, y, batch_size=128, epochs=100, verbose=1, callbacks=[checkpoint])\n",
    "pickle.dump(tokenizer, open('tokenizer_model.pkl','wb'))\n",
    "model.save('trained_recipe_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = load_model('model/trained_recipe_model.h5')\n",
    "with open('model/tokenizer_model.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentence(trained_model, tokenizer, seq_length, 'i made bread', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group recipes by yeast, levain and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
